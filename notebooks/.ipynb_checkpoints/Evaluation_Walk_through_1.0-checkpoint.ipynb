{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One run full walktrhough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do the full walk through on the large data set\n",
    "* Refactor the source code and bring it to individual scripts\n",
    "* Ensure a full run with one click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your base path is at: Lecture_Covid_19_data_analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check some parameters\n",
    "## depending where you launch your notebook, the relative path might not work\n",
    "## you should start the notebook server from your base path\n",
    "## when opening the notebook, typically your path will be ../ads_covid-19/notebooks\n",
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Update all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-53dcbe8cfefe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mget_johns_hopkins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mget_current_data_germany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-53dcbe8cfefe>\u001b[0m in \u001b[0;36mget_johns_hopkins\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m                          \u001b[0mshell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                          \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                          stderr = subprocess.PIPE )\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgit_pull\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    754\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    757\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1153\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1156\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid"
     ]
    }
   ],
   "source": [
    "# %load src/data/get_data.py\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_johns_hopkins():\n",
    "    ''' Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure\n",
    "    '''\n",
    "    git_pull = subprocess.Popen( \"/mingw64/bin/git pull\" ,\n",
    "                         cwd = os.path.dirname( 'data/raw/COVID-19/' ),\n",
    "                         shell = True,\n",
    "                         stdout = subprocess.PIPE,\n",
    "                         stderr = subprocess.PIPE )\n",
    "    (out, error) = git_pull.communicate()\n",
    "\n",
    "\n",
    "    print(\"Error : \" + str(error))\n",
    "    print(\"out : \" + str(out))\n",
    "\n",
    "\n",
    "def get_current_data_germany():\n",
    "    ''' Get current data from germany, attention API endpoint not too stable\n",
    "        Result data frame is stored as pd.DataFrame\n",
    "\n",
    "    '''\n",
    "    # 16 states\n",
    "    #data=requests.get('https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/Coronaf%C3%A4lle_in_den_Bundesl%C3%A4ndern/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json')\n",
    "\n",
    "    # 400 regions / Landkreise\n",
    "    data=requests.get('https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json')\n",
    "\n",
    "    json_object=json.loads(data.content)\n",
    "    full_list=[]\n",
    "    for pos,each_dict in enumerate (json_object['features'][:]):\n",
    "        full_list.append(each_dict['attributes'])\n",
    "\n",
    "    pd_full_list=pd.DataFrame(full_list)\n",
    "    pd_full_list.to_csv('data/raw/NPGEO/GER_state_data.csv',sep=';')\n",
    "    print(' Number of regions rows: '+str(pd_full_list.shape[0]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_johns_hopkins()\n",
    "    get_current_data_germany()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 54530\n",
      " Latest date is: 2020-08-13 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# %load src/data/process_JH_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def store_relational_JH_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "\n",
    "    data_path='..\\\\data\\\\raw\\\\COVID-19\\\\csse_covid_19_data\\\\csse_covid_19_time_series\\\\time_series_covid19_confirmed_global.csv'\n",
    "    pd_raw=pd.read_csv(data_path)\n",
    "\n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',\n",
    "                      'Province/State':'state'})\n",
    "\n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')\n",
    "\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "\n",
    "\n",
    "    pd_relational_model=pd_data_base.set_index(['state','country']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                  )\n",
    "\n",
    "    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "\n",
    "    pd_relational_model.to_csv('..\\\\data\\\\processed\\\\COVID_relational_confirmed.csv',sep=';',index=False)\n",
    "    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_model.date)))\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_JH_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Filter and Doubling Rate Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pramod\\Anaconda3\\lib\\site-packages\\scipy\\signal\\_arraytools.py:45: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  b = a[a_slice]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date state  country  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "29720 2020-08-09    no  Germany   217288.0            217687.0    397.061661   \n",
      "29721 2020-08-10    no  Germany   218508.0            218619.6    271.110696   \n",
      "29722 2020-08-11    no  Germany   219540.0            219695.2    194.001184   \n",
      "29723 2020-08-12    no  Germany   220859.0            220928.9    186.844463   \n",
      "29724 2020-08-13    no  Germany   222281.0            222162.6    161.177186   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "29720             243.698141  \n",
      "29721             237.557617  \n",
      "29722             217.774392  \n",
      "29723             190.315593  \n",
      "29724             179.078301  \n"
     ]
    }
   ],
   "source": [
    "# %load src/features/build_features.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5):\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    #print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    #df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['state','country']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv('..\\\\data\\\\processed\\\\COVID_relational_confirmed.csv',sep=';',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    #test_structure=pd_JH_data[((pd_JH_data['country']=='US')|\n",
    "    #                  (pd_JH_data['country']=='Germany'))]\n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=pd_result_larg['confirmed']>100\n",
    "    pd_result_larg['confirmed_filtered_DR']=pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('..\\\\data\\\\processed\\\\COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['country']=='Germany'].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date state country  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "51860 2020-08-09    no      US  5044864.0           5044031.2     96.884837   \n",
      "51861 2020-08-10    no      US  5094400.0           5095162.4    104.606172   \n",
      "51862 2020-08-11    no      US  5141208.0           5145347.4    105.735503   \n",
      "51863 2020-08-12    no      US  5197411.0           5196446.5     99.879424   \n",
      "51864 2020-08-13    no      US  5248854.0           5247545.6     96.535391   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "51860              94.514426  \n",
      "51861              98.267616  \n",
      "51862             100.573196  \n",
      "51863             101.608290  \n",
      "51864             101.693503  \n"
     ]
    }
   ],
   "source": [
    "print(pd_result_larg[pd_result_larg['country']=='US'].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visual Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIR_model(SIR,beta,gamma, N0):\n",
    "    ''' Simple SIR model\n",
    "        S: susceptible population\n",
    "        I: infected people\n",
    "        R: recovered people\n",
    "        beta: \n",
    "        \n",
    "        overall condition is that the sum of changes (differnces) sum up to 0\n",
    "        dS+dI+dR=0\n",
    "        S+I+R= N (constant size of population)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    S,I,R=SIR\n",
    "    dS_dt=-beta*S*I/N0          #S*I is the \n",
    "    dI_dt=beta*S*I/N0-gamma*I\n",
    "    dR_dt=gamma*I\n",
    "    return([dS_dt,dI_dt,dR_dt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pop = pd.read_csv('..//data/raw//country_populations.csv')[['Country Name','2019 [YR2019]']]\n",
    "pop.rename(columns={'Country Name': 'country', '2019 [YR2019]': 'count'}, inplace = True)\n",
    "pop.country = pop.country.map(lambda x: str(x))\n",
    "df_input_large=pd.read_csv('..\\\\data\\\\processed\\\\COVID_final_set.csv',sep=';')\n",
    "common_countries = set(df_input_large.country.map(lambda x: x.lower())).intersection(set(pop.country.map(lambda x: x.lower())))\n",
    "df_input_large_filtered = df_input_large[df_input_large.country.map(lambda x: x.lower()).isin(common_countries)]\n",
    "\n",
    "pop = pop[pop.country.map(lambda x: x.lower()).isin(common_countries)]\n",
    "country_vs_pop = {row['country']:row['count'] for row in pop.to_dict('records')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pramod\\Desktop\\TUKL\\course books\\sem4\\EDS\\Lecture_Covid_19_data_analysis\\notebooks\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pramod\\Anaconda3\\envs\\preqa\\lib\\site-packages\\ipykernel_launcher.py:135: FutureWarning:\n",
      "\n",
      "Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load src/visualization/visualize.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dash\n",
    "dash.__version__\n",
    "import plotly.express as px\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "df_input_large=pd.read_csv('..\\\\data\\\\processed\\\\COVID_final_set.csv',sep=';')\n",
    "df_input_large = df_input_large_filtered\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "app = dash.Dash()\n",
    "app.layout = html.Div([\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    #  Applied Data Science on COVID-19 data\n",
    "\n",
    "    Goal of the project is to teach data science by applying a cross industry standard process,\n",
    "    it covers the full walkthrough of: automated data gathering, data transformations,\n",
    "    filtering and machine learning to approximating the doubling time, and\n",
    "    (static) deployment of responsive dashboard.\n",
    "\n",
    "    '''),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    ## Multi-Select Country for visualization\n",
    "    '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='country_drop_down',\n",
    "        options=[ {'label': each,'value':each} for each in df_input_large['country'].unique()],\n",
    "        value=['US', 'Germany','Italy'], # which are pre-selected\n",
    "        multi=True\n",
    "    ),\n",
    "\n",
    "    \n",
    "\n",
    "    dcc.Tabs(id='main_tab', value='main_tab', children=[\n",
    "        dcc.Tab(id='tab1', label='Confirmed Cases Analytics', value='tab1', children=[\n",
    "                dcc.Markdown('''\n",
    "            ## Select Timeline of confirmed COVID-19 cases or the approximated doubling time\n",
    "            '''),\n",
    "                 dcc.Dropdown(\n",
    "                        id='doubling_time',\n",
    "                        options=[\n",
    "                            {'label': 'Timeline Confirmed ', 'value': 'confirmed'},\n",
    "                            {'label': 'Timeline Confirmed Filtered', 'value': 'confirmed_filtered'},\n",
    "                            {'label': 'Timeline Doubling Rate', 'value': 'confirmed_DR'},\n",
    "                            {'label': 'Timeline Doubling Rate Filtered', 'value': 'confirmed_filtered_DR'},\n",
    "                        ],\n",
    "                        value='confirmed',\n",
    "                        multi=False\n",
    "                        ),\n",
    "                    dcc.Graph(figure=fig, id='main_window_slope')\n",
    "                ]),\n",
    "        #SIR simulation graph\n",
    "        dcc.Tab(id='tab2', label='Infection Spread Model - SIR', value='tab2', children=[\n",
    "              dcc.Markdown('''\n",
    "              Initialise Infection and Recovery rates\n",
    "              '''),\n",
    "           html.Div([\"Min Infection Rate: \", dcc.Input(id='beta_min', value = .11,  placeholder='min infection rate', type='number'),\n",
    "                    html.Br(), html.Br(),\n",
    "                    \"Max Infection Rate: \", dcc.Input(id='beta_max', value = .4, placeholder='max infection rate', type='number'),\n",
    "                    html.Br(), html.Br(),\n",
    "                    \"# Days without measures: \", dcc.Input(id='t_init', value=28,  placeholder='initial time period ', type='number'),\n",
    "                    html.Br(), html.Br(),\n",
    "                     \"# Days with hard measures : \", dcc.Input(id='t_intro', value = 14, placeholder='time to introduce measures', type='number'),\n",
    "                     html.Br(), html.Br(),\n",
    "                     \"# Days held with measures : \", dcc.Input(id='t_hold', value=21,  placeholder='hold time', type='number'),\n",
    "                     html.Br(), html.Br(),\n",
    "                     \"# Days with relaxed measures : \", dcc.Input(id='t_relax', value = 21, placeholder='relaxed time', type='number'),\n",
    "                     html.Br(), html.Br(),\n",
    "                     \"# Days repeated with hard measures : \", dcc.Input(id='t_repeat', value = 31, placeholder='repeated hard measures', type='number'),\n",
    "                     html.Br(), html.Br(),\n",
    "                     \"Recovery Rate : \", dcc.Input(id='gamma', value = .1, placeholder='recovery rate', type='number'),\n",
    "                     html.Br(), html.Br(),\n",
    "                     html.Button('Submit', id='submit', n_clicks = 0)\n",
    "                    ]\n",
    "                   \n",
    "                   ),\n",
    "            dcc.Graph(figure=fig, id='sir_graph')\n",
    "            \n",
    "            \n",
    "        ])\n",
    "    ])\n",
    "])\n",
    "\n",
    "g_country_list = []\n",
    "\n",
    "@app.callback(\n",
    "    Output('sir_graph', 'figure'),\n",
    "    [Input('submit', 'n_clicks')],\n",
    "    [State('beta_min', 'value'),\n",
    "    State('beta_max', 'value'),\n",
    "    State('t_init', 'value'),\n",
    "    State('t_intro', 'value'),\n",
    "    State('t_hold', 'value'),\n",
    "    State('t_relax', 'value'),\n",
    "    State('t_repeat', 'value'),\n",
    "    State('gamma', 'value')])\n",
    "def sir_graph(n_clicks, beta_min, beta_max, t_init, t_intro, t_hold, t_relax, t_repeat,  gamma):\n",
    "    global g_country_list\n",
    "    if n_clicks is None or n_clicks ==0:\n",
    "        return {\n",
    "            'data': [],\n",
    "            'layout': dict (\n",
    "                width=1280,\n",
    "                height=720,\n",
    "                xaxis = dict(showticklabels=False, showgrid=False, zeroline = False),\n",
    "                yaxis = dict(showticklabels=False, showgrid=False, zeroline = False),\n",
    "        )\n",
    "    }\n",
    "    pd_beta=np.concatenate((np.array(int(t_init)*[beta_max]),\n",
    "                       np.linspace(beta_max,beta_min,int(t_intro)),\n",
    "                       np.array(int(t_hold)*[beta_min]),\n",
    "                        np.linspace(beta_min,beta_max,int(t_relax)),\n",
    "                        np.linspace(beta_min,beta_max,int(t_repeat)),\n",
    "                       ))\n",
    "    t_phases=np.array([t_init, t_intro, t_hold, t_relax, t_repeat]).cumsum()\n",
    "    fig = go.Figure()\n",
    "    for each in g_country_list:\n",
    "        N0 = int(country_vs_pop[each])\n",
    "        #considering confirmed cases as the initial infected people\n",
    "        df_filter_by_country = df_input_large_filtered[df_input_large_filtered.country == each]\n",
    "        I0=df_filter_by_country.confirmed.iloc[df_filter_by_country.confirmed.nonzero()[0][0]]\n",
    "        S0=N0-I0\n",
    "        R0=0\n",
    "        propagation_rates=pd.DataFrame(columns  = {'susceptible':S0, 'infected':I0, 'recovered':R0}, index = [0])\n",
    "        SIR=np.array([S0,I0,R0])\n",
    "        for each_beta in pd_beta:\n",
    "            new_delta_vec=SIR_model(SIR,each_beta,gamma, N0)\n",
    "            SIR=SIR+new_delta_vec\n",
    "            propagation_rates=propagation_rates.append({'susceptible':SIR[0],\n",
    "                                                    'infected':SIR[1],\n",
    "                                                  'recovered':SIR[2]}, ignore_index=True)\n",
    "#         print(propagation_rates.infected)\n",
    "        fig.add_trace(go.Scatter(x=propagation_rates.index,y=propagation_rates.infected, name = each ))\n",
    "                        \n",
    "                \n",
    "    fig = update_layout(t_phases, fig)\n",
    "    return fig\n",
    "\n",
    "def update_layout(t_phases, fig):\n",
    "    color = 'red'\n",
    "    fig.update_layout(\n",
    "    title='Scenario SIR simulations  (demonstration purposes only)',\n",
    "    width=1280,\n",
    "    height=720,\n",
    "    xaxis = dict(showgrid=False, title = 'time in days'),\n",
    "    yaxis_type=\"log\",\n",
    "    yaxis = dict(showgrid=False),\n",
    "    shapes=[dict(type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            x0=0,\n",
    "            y0=0,\n",
    "            x1=t_phases[0],\n",
    "            y1=1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.0,\n",
    "            layer=\"below\",\n",
    "            line_width=0),\n",
    "        dict(type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            x0=t_phases[0],\n",
    "            y0=0,\n",
    "            x1=t_phases[1],\n",
    "            y1=1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.3,\n",
    "            layer=\"below\",\n",
    "            line_width=0),\n",
    "        dict(type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            x0=t_phases[1],\n",
    "            y0=0,\n",
    "            x1=t_phases[2],\n",
    "            y1=1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.4,\n",
    "            layer=\"below\",\n",
    "            line_width=0),\n",
    "        dict(type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            x0=t_phases[2],\n",
    "            y0=0,\n",
    "            x1=t_phases[3],\n",
    "            y1=1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.1,\n",
    "            layer=\"below\",\n",
    "            line_width=0),\n",
    "          dict(type=\"rect\",\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            x0=t_phases[3],\n",
    "            y0=0,\n",
    "            x1= t_phases[4],\n",
    "            y1=1,\n",
    "            fillcolor=color,\n",
    "            opacity=0.3,\n",
    "            layer=\"below\",\n",
    "            line_width=0)          \n",
    "    ])\n",
    "    fig.add_trace(go.Scatter(\n",
    "    x=[10, t_phases[0]+10, t_phases[1]+10, t_phases[2]+10, t_phases[3]+10],\n",
    "    y=[10, 100, 200, 175, 10],\n",
    "    text=[\"no measures \",\n",
    "          \"hard measures\",\n",
    "          \"hold measures\",\n",
    "          \"relax measures\",\n",
    "          \"repeat hard measures\"],\n",
    "    mode=\"text\",\n",
    "))\n",
    "    return fig\n",
    "\n",
    "    \n",
    "\n",
    "@app.callback(\n",
    "    [Output('main_window_slope', 'figure')],\n",
    "    [Input('country_drop_down', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "def update_figure(country_list,show_doubling):\n",
    "    global g_country_list\n",
    "    if 'doubling_rate' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days (larger numbers are better #stayathome)'\n",
    "              }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source johns hopkins csse, log-scale)'\n",
    "              }\n",
    "\n",
    "    \n",
    "    traces = []\n",
    "    g_country_list = country_list\n",
    "    for each in country_list:\n",
    "        \n",
    "        df_plot=df_input_large[df_input_large['country']==each]\n",
    "\n",
    "        if show_doubling=='doubling_rate_filtered':\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.sum).reset_index()\n",
    "       #print(show_doubling)\n",
    "\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return [{\n",
    "            'data': traces,\n",
    "            'layout': dict (\n",
    "                width=400,\n",
    "                height=200,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(\n",
    "                            size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        )\n",
    "    }]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
